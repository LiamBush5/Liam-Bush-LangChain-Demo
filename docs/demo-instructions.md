# Senior TSE - Prompt

### Background

We want to see how you think like a builder and support engineer â€” how you learn tools, experiment with features, and communicate technical feedback. This task is designed to assess your ability to understand and work with  **LangSmith** , our evaluation and observability platform for LLM apps.

### **What Youâ€™ll Be Doing**

Your task is to **run an evaluation experiment in LangSmith** â€” both via the **SDK** and the  **UI** .

LangSmith experiments allow users to test their LLM applications against datasets and evaluate outputs using scoring functions, including LLM-as-judge setups for attributes like correctness, relevance, or completeness.

Weâ€™re not looking for perfection or polish â€” weâ€™re looking for signal in your thinking:

* How do you reason about LangSmithâ€™s evaluation flow?
* Do you understand what the experiment is testing and why it matters?
* Can you identify whatâ€™s working, whatâ€™s confusing, and how it could improve?

### Task

1. Pick or create a small dataset (can be synthetic) to evaluate an LLM-powered chain or app.
2. Run an experiment in  **LangSmith** , using both:
   * The  **LangSmith UI** , and
   * The **LangSmith SDK** (code-based)
3. Make the experiment as realistic as possible â€” think like a developer using LangSmith to debug or improve their app.

### Resources

* ğŸ“˜ [LangSmith Evaluation Docs](https://docs.smith.langchain.com/evaluation)
* ğŸ“ [LangSmith Academy â€“ Module 2: Evaluation](https://academy.langchain.com/courses/intro-to-langsmith)

### What to share

* A **short walkthrough (~10 min)** to be covered live on our next call explaining:
  * What you built
  * What you learned or found surprising
  * What might confuse a new user
* A link to your **code snippets or repo**
* Optionally, a **â€œfriction logâ€** â€” list anything unclear, buggy, or unintuitive youâ€™d flag to the engineering team if you were already on the job
