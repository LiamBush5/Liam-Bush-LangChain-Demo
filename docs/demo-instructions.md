# Senior TSE - Prompt

### Background

We want to see how you think like a builder and support engineer — how you learn tools, experiment with features, and communicate technical feedback. This task is designed to assess your ability to understand and work with  **LangSmith** , our evaluation and observability platform for LLM apps.

### **What You’ll Be Doing**

Your task is to **run an evaluation experiment in LangSmith** — both via the **SDK** and the  **UI** .

LangSmith experiments allow users to test their LLM applications against datasets and evaluate outputs using scoring functions, including LLM-as-judge setups for attributes like correctness, relevance, or completeness.

We’re not looking for perfection or polish — we’re looking for signal in your thinking:

* How do you reason about LangSmith’s evaluation flow?
* Do you understand what the experiment is testing and why it matters?
* Can you identify what’s working, what’s confusing, and how it could improve?

### Task

1. Pick or create a small dataset (can be synthetic) to evaluate an LLM-powered chain or app.
2. Run an experiment in  **LangSmith** , using both:
   * The  **LangSmith UI** , and
   * The **LangSmith SDK** (code-based)
3. Make the experiment as realistic as possible — think like a developer using LangSmith to debug or improve their app.

### Resources

* 📘 [LangSmith Evaluation Docs](https://docs.smith.langchain.com/evaluation)
* 🎓 [LangSmith Academy – Module 2: Evaluation](https://academy.langchain.com/courses/intro-to-langsmith)

### What to share

* A **short walkthrough (~10 min)** to be covered live on our next call explaining:
  * What you built
  * What you learned or found surprising
  * What might confuse a new user
* A link to your **code snippets or repo**
* Optionally, a **“friction log”** — list anything unclear, buggy, or unintuitive you’d flag to the engineering team if you were already on the job
